{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4283 disorders.\n",
      "Total unique phenotypes: 8600\n",
      "Total unique diseases: 4283\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "\n",
    "# Load the disorders JSON file\n",
    "def load_disorders(json_file):\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        disorders = json.load(f)\n",
    "    return disorders\n",
    "\n",
    "# Helper function to parse frequency strings into a numeric value\n",
    "def parse_frequency(freq_str):\n",
    "    if freq_str is None:\n",
    "        return 0.5\n",
    "    if \"Very frequent\" in freq_str:\n",
    "        return 0.9\n",
    "    elif \"Frequent\" in freq_str:\n",
    "        return 0.6\n",
    "    elif \"Occasional\" in freq_str:\n",
    "        return 0.2\n",
    "    else:\n",
    "        return 0.5\n",
    "\n",
    "disorders = load_disorders('./disorders.json')\n",
    "print(\"Loaded\", len(disorders), \"disorders.\")\n",
    "\n",
    "# Build dictionaries for nodes and accumulate disease->phenotype associations\n",
    "phenotype_dict = {}  # key: hpo_id, value: {'term': ..., 'frequency': ...}\n",
    "disease_dict = {}    # key: disease id (using OrphaCode), value: {'name': ...}\n",
    "disease_to_pheno_edges = []  # List of tuples: (disease_id, hpo_id, weight)\n",
    "\n",
    "for disorder in disorders:\n",
    "    disease_id = disorder.get('orpha_code')\n",
    "    disease_name = disorder.get('name')\n",
    "    if disease_id not in disease_dict:\n",
    "        disease_dict[disease_id] = {'name': disease_name}\n",
    "    \n",
    "    for pheno in disorder.get('phenotypes', []):\n",
    "        hpo_id = pheno.get('hpo_id')\n",
    "        hpo_term = pheno.get('HPOTerm') or pheno.get('hpo_term')\n",
    "        freq_str = pheno.get('frequency')\n",
    "        freq_val = parse_frequency(freq_str)\n",
    "        \n",
    "        if hpo_id not in phenotype_dict:\n",
    "            phenotype_dict[hpo_id] = {'term': hpo_term, 'frequency': freq_val}\n",
    "        \n",
    "        # Append the disease-to-phenotype edge with the frequency as weight\n",
    "        disease_to_pheno_edges.append((disease_id, hpo_id, freq_val))\n",
    "\n",
    "print(\"Total unique phenotypes:\", len(phenotype_dict))\n",
    "print(\"Total unique diseases:\", len(disease_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"orpha_code\": \"58\",\n",
      "        \"name\": \"Alexander disease\",\n",
      "        \"type\": \"Disease\",\n",
      "        \"phenotypes\": [\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0000256\",\n",
      "                \"hpo_term\": \"Macrocephaly\",\n",
      "                \"frequency\": \"Very frequent (99-80%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0001249\",\n",
      "                \"hpo_term\": \"Intellectual disability\",\n",
      "                \"frequency\": \"Very frequent (99-80%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0001250\",\n",
      "                \"hpo_term\": \"Seizure\",\n",
      "                \"frequency\": \"Very frequent (99-80%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0001257\",\n",
      "                \"hpo_term\": \"Spasticity\",\n",
      "                \"frequency\": \"Very frequent (99-80%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0001274\",\n",
      "                \"hpo_term\": \"Agenesis of corpus callosum\",\n",
      "                \"frequency\": \"Very frequent (99-80%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0001347\",\n",
      "                \"hpo_term\": \"Hyperreflexia\",\n",
      "                \"frequency\": \"Very frequent (99-80%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0001355\",\n",
      "                \"hpo_term\": \"Megalencephaly\",\n",
      "                \"frequency\": \"Very frequent (99-80%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0001508\",\n",
      "                \"hpo_term\": \"Failure to thrive\",\n",
      "                \"frequency\": \"Very frequent (99-80%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0002007\",\n",
      "                \"hpo_term\": \"Frontal bossing\",\n",
      "                \"frequency\": \"Very frequent (99-80%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0002017\",\n",
      "                \"hpo_term\": \"Nausea and vomiting\",\n",
      "                \"frequency\": \"Very frequent (99-80%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0002167\",\n",
      "                \"hpo_term\": \"Abnormality of speech or vocalization\",\n",
      "                \"frequency\": \"Very frequent (99-80%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0002169\",\n",
      "                \"hpo_term\": \"Clonus\",\n",
      "                \"frequency\": \"Very frequent (99-80%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0002353\",\n",
      "                \"hpo_term\": \"EEG abnormality\",\n",
      "                \"frequency\": \"Very frequent (99-80%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0002360\",\n",
      "                \"hpo_term\": \"Sleep abnormality\",\n",
      "                \"frequency\": \"Very frequent (99-80%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0002650\",\n",
      "                \"hpo_term\": \"Scoliosis\",\n",
      "                \"frequency\": \"Very frequent (99-80%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0007256\",\n",
      "                \"hpo_term\": \"Abnormal pyramidal sign\",\n",
      "                \"frequency\": \"Very frequent (99-80%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0100729\",\n",
      "                \"hpo_term\": \"Large face\",\n",
      "                \"frequency\": \"Very frequent (99-80%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0000496\",\n",
      "                \"hpo_term\": \"Abnormality of eye movement\",\n",
      "                \"frequency\": \"Frequent (79-30%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0000508\",\n",
      "                \"hpo_term\": \"Ptosis\",\n",
      "                \"frequency\": \"Frequent (79-30%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0000639\",\n",
      "                \"hpo_term\": \"Nystagmus\",\n",
      "                \"frequency\": \"Frequent (79-30%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0000651\",\n",
      "                \"hpo_term\": \"Diplopia\",\n",
      "                \"frequency\": \"Frequent (79-30%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0000712\",\n",
      "                \"hpo_term\": \"Emotional lability\",\n",
      "                \"frequency\": \"Frequent (79-30%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0000716\",\n",
      "                \"hpo_term\": \"Depression\",\n",
      "                \"frequency\": \"Frequent (79-30%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0000975\",\n",
      "                \"hpo_term\": \"Hyperhidrosis\",\n",
      "                \"frequency\": \"Frequent (79-30%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0001251\",\n",
      "                \"hpo_term\": \"Ataxia\",\n",
      "                \"frequency\": \"Frequent (79-30%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0001260\",\n",
      "                \"hpo_term\": \"Dysarthria\",\n",
      "                \"frequency\": \"Frequent (79-30%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0001288\",\n",
      "                \"hpo_term\": \"Gait disturbance\",\n",
      "                \"frequency\": \"Frequent (79-30%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0001337\",\n",
      "                \"hpo_term\": \"Tremor\",\n",
      "                \"frequency\": \"Frequent (79-30%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0001618\",\n",
      "                \"hpo_term\": \"Dysphonia\",\n",
      "                \"frequency\": \"Frequent (79-30%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0002015\",\n",
      "                \"hpo_term\": \"Dysphagia\",\n",
      "                \"frequency\": \"Frequent (79-30%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0002019\",\n",
      "                \"hpo_term\": \"Constipation\",\n",
      "                \"frequency\": \"Frequent (79-30%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0002045\",\n",
      "                \"hpo_term\": \"Hypothermia\",\n",
      "                \"frequency\": \"Frequent (79-30%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0002381\",\n",
      "                \"hpo_term\": \"Aphasia\",\n",
      "                \"frequency\": \"Frequent (79-30%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0002445\",\n",
      "                \"hpo_term\": \"Tetraplegia\",\n",
      "                \"frequency\": \"Frequent (79-30%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0002514\",\n",
      "                \"hpo_term\": \"Cerebral calcification\",\n",
      "                \"frequency\": \"Frequent (79-30%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0002615\",\n",
      "                \"hpo_term\": \"Hypotension\",\n",
      "                \"frequency\": \"Frequent (79-30%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0002808\",\n",
      "                \"hpo_term\": \"Kyphosis\",\n",
      "                \"frequency\": \"Frequent (79-30%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0010535\",\n",
      "                \"hpo_term\": \"Sleep apnea\",\n",
      "                \"frequency\": \"Frequent (79-30%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0010628\",\n",
      "                \"hpo_term\": \"Facial palsy\",\n",
      "                \"frequency\": \"Frequent (79-30%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0100247\",\n",
      "                \"hpo_term\": \"Recurrent singultus\",\n",
      "                \"frequency\": \"Frequent (79-30%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0000218\",\n",
      "                \"hpo_term\": \"High palate\",\n",
      "                \"frequency\": \"Occasional (29-5%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0000238\",\n",
      "                \"hpo_term\": \"Hydrocephalus\",\n",
      "                \"frequency\": \"Occasional (29-5%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0000470\",\n",
      "                \"hpo_term\": \"Short neck\",\n",
      "                \"frequency\": \"Occasional (29-5%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0000819\",\n",
      "                \"hpo_term\": \"Diabetes mellitus\",\n",
      "                \"frequency\": \"Occasional (29-5%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0000821\",\n",
      "                \"hpo_term\": \"Hypothyroidism\",\n",
      "                \"frequency\": \"Occasional (29-5%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0000822\",\n",
      "                \"hpo_term\": \"Hypertension\",\n",
      "                \"frequency\": \"Occasional (29-5%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0000826\",\n",
      "                \"hpo_term\": \"Precocious puberty\",\n",
      "                \"frequency\": \"Occasional (29-5%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0000938\",\n",
      "                \"hpo_term\": \"Osteopenia\",\n",
      "                \"frequency\": \"Occasional (29-5%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0001252\",\n",
      "                \"hpo_term\": \"Hypotonia\",\n",
      "                \"frequency\": \"Occasional (29-5%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0001324\",\n",
      "                \"hpo_term\": \"Muscle weakness\",\n",
      "                \"frequency\": \"Occasional (29-5%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0001645\",\n",
      "                \"hpo_term\": \"Sudden cardiac death\",\n",
      "                \"frequency\": \"Occasional (29-5%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0002072\",\n",
      "                \"hpo_term\": \"Chorea\",\n",
      "                \"frequency\": \"Occasional (29-5%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0002093\",\n",
      "                \"hpo_term\": \"Respiratory insufficiency\",\n",
      "                \"frequency\": \"Occasional (29-5%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0002376\",\n",
      "                \"hpo_term\": \"Developmental regression\",\n",
      "                \"frequency\": \"Occasional (29-5%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0002383\",\n",
      "                \"hpo_term\": \"Infectious encephalitis\",\n",
      "                \"frequency\": \"Occasional (29-5%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0002410\",\n",
      "                \"hpo_term\": \"Aqueductal stenosis\",\n",
      "                \"frequency\": \"Occasional (29-5%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0002607\",\n",
      "                \"hpo_term\": \"Bowel incontinence\",\n",
      "                \"frequency\": \"Occasional (29-5%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0003307\",\n",
      "                \"hpo_term\": \"Hyperlordosis\",\n",
      "                \"frequency\": \"Occasional (29-5%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0007481\",\n",
      "                \"hpo_term\": \"Hyperpigmented nevi\",\n",
      "                \"frequency\": \"Occasional (29-5%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0012332\",\n",
      "                \"hpo_term\": \"Abnormal autonomic nervous system physiology\",\n",
      "                \"frequency\": \"Occasional (29-5%)\"\n",
      "            },\n",
      "            {\n",
      "                \"hpo_id\": \"HP:0100716\",\n",
      "                \"hpo_term\": \"Self-injurious behavior\",\n",
      "                \"frequency\": \"Occasional (29-5%)\"\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('disorders.json','r') as f:\n",
    "    data=json.load(f)\n",
    "print(json.dumps(data[:1], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting obonet\n",
      "  Using cached obonet-1.1.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: networkx in /Users/sriramsohan/miniforge3/envs/RL_env/lib/python3.10/site-packages (from obonet) (3.4.2)\n",
      "Using cached obonet-1.1.0-py3-none-any.whl (9.1 kB)\n",
      "Installing collected packages: obonet\n",
      "Successfully installed obonet-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install obonet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heterogeneous graph created:\n",
      "HeteroData(\n",
      "  phenotype={ x=[8600, 2] },\n",
      "  disease={ x=[4283, 2] },\n",
      "  (disease, has_phenotype, phenotype)={\n",
      "    edge_index=[2, 114961],\n",
      "    edge_attr=[114961, 1],\n",
      "  },\n",
      "  (phenotype, associated_with, disease)={\n",
      "    edge_index=[2, 114961],\n",
      "    edge_attr=[114961, 1],\n",
      "  },\n",
      "  (phenotype, is_a, phenotype)={\n",
      "    edge_index=[2, 8609],\n",
      "    edge_attr=[8609, 1],\n",
      "  }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "from torch_geometric.data import HeteroData\n",
    "import math\n",
    "import obonet\n",
    "\n",
    "# (Optional) Placeholder for HPO hierarchy edges; replace with real data if available.\n",
    "def get_hpo_hierarchy_edges(obo_file='hp.obo'):\n",
    "    \n",
    "    graph = obonet.read_obo(obo_file)\n",
    "    hierarchy_edges=[]\n",
    "    for node in graph.nodes():\n",
    "        # Each node's 'is_a' attribute contains its parent terms (if any)\n",
    "        if 'is_a' in graph.nodes[node]:\n",
    "            for parent in graph.nodes[node]['is_a']:\n",
    "                hierarchy_edges.append((node,parent))\n",
    "    return hierarchy_edges\n",
    "\n",
    "\n",
    "\n",
    "# Create the HeteroData object\n",
    "data = HeteroData()\n",
    "\n",
    "# --- Add Phenotype Nodes ---\n",
    "# Each phenotype node gets a feature vector: [frequency, IC] where IC = -log(frequency)\n",
    "phenotype_list = list(phenotype_dict.keys())\n",
    "phenotype_features = []\n",
    "for hpo_id in phenotype_list:\n",
    "    freq = phenotype_dict[hpo_id]['frequency']\n",
    "    ic = -math.log(freq)\n",
    "    phenotype_features.append([freq, ic])\n",
    "data['phenotype'].x = torch.tensor(phenotype_features, dtype=torch.float)\n",
    "\n",
    "# --- Add Disease Nodes ---\n",
    "# Create a dummy feature vector for disease nodes (initially 1-dimensional)\n",
    "disease_list = list(disease_dict.keys())\n",
    "disease_features = [[1.0] for _ in disease_list]\n",
    "disease_features = torch.tensor(disease_features, dtype=torch.float)\n",
    "# Expand disease node features from 1D to 2D to match phenotype nodes (e.g., simply repeat the feature)\n",
    "if disease_features.shape[1] == 1:\n",
    "    disease_features = disease_features.repeat(1, 2)\n",
    "data['disease'].x = disease_features\n",
    "\n",
    "# Create mapping from IDs to indices for each node type\n",
    "pheno_to_idx = {hpo_id: i for i, hpo_id in enumerate(phenotype_list)}\n",
    "disease_to_idx = {d_id: i for i, d_id in enumerate(disease_list)}\n",
    "\n",
    "# --- Add Edges: Disease -> Phenotype ---\n",
    "src, dst, edge_weights = [], [], []\n",
    "for disease_id, hpo_id, weight in disease_to_pheno_edges:\n",
    "    if disease_id in disease_to_idx and hpo_id in pheno_to_idx:\n",
    "        src.append(disease_to_idx[disease_id])\n",
    "        dst.append(pheno_to_idx[hpo_id])\n",
    "        edge_weights.append([weight])\n",
    "data['disease', 'has_phenotype', 'phenotype'].edge_index = torch.tensor([src, dst], dtype=torch.long)\n",
    "data['disease', 'has_phenotype', 'phenotype'].edge_attr = torch.tensor(edge_weights, dtype=torch.float)\n",
    "\n",
    "# --- Add Reverse Edges: Phenotype -> Disease ---\n",
    "data['phenotype', 'associated_with', 'disease'].edge_index = torch.tensor([dst, src], dtype=torch.long)\n",
    "data['phenotype', 'associated_with', 'disease'].edge_attr = torch.tensor(edge_weights, dtype=torch.float)\n",
    "\n",
    "# --- (Optional) Add HPO Hierarchy Edges: Phenotype -> Phenotype ---\n",
    "hpo_hierarchy_edges = get_hpo_hierarchy_edges('hp.obo')\n",
    "if hpo_hierarchy_edges:\n",
    "    src_h, dst_h, hierarchy_weights = [], [], []\n",
    "    for child, parent in hpo_hierarchy_edges:\n",
    "        if child in pheno_to_idx and parent in pheno_to_idx:\n",
    "            src_h.append(pheno_to_idx[child])\n",
    "            dst_h.append(pheno_to_idx[parent])\n",
    "            hierarchy_weights.append([1.0])\n",
    "    data['phenotype', 'is_a', 'phenotype'].edge_index = torch.tensor([src_h, dst_h], dtype=torch.long)\n",
    "    data['phenotype', 'is_a', 'phenotype'].edge_attr = torch.tensor(hierarchy_weights, dtype=torch.float)\n",
    "\n",
    "print(\"Heterogeneous graph created:\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1130 weakly connected components.\n",
      "Largest component size: 7018\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Check if the HPO hierarchy relation exists in the heterogeneous graph\n",
    "if ('phenotype', 'is_a', 'phenotype') in data.edge_index_dict:\n",
    "    hier_edge_index = data['phenotype', 'is_a', 'phenotype'].edge_index\n",
    "    hier_edge_attr = data['phenotype', 'is_a', 'phenotype'].edge_attr\n",
    "    hier_edge_index_np = hier_edge_index.cpu().numpy()\n",
    "    \n",
    "    # Build a full directed graph for all phenotype nodes using the \"is_a\" edges.\n",
    "    H_full = nx.DiGraph()\n",
    "    num_nodes = data['phenotype'].num_nodes\n",
    "    for i in range(num_nodes):\n",
    "        freq, ic = data['phenotype'].x[i].tolist()\n",
    "        H_full.add_node(i, label=f\"f:{freq:.2f}, ic:{ic:.2f}\")\n",
    "    \n",
    "    for j in range(hier_edge_index_np.shape[1]):\n",
    "        child = hier_edge_index_np[0, j]\n",
    "        parent = hier_edge_index_np[1, j]\n",
    "        weight = hier_edge_attr[j].item()\n",
    "        H_full.add_edge(child, parent, weight=float(f\"{weight:.4f}\"))\n",
    "    \n",
    "    # Find weakly connected components in the directed graph\n",
    "    components = list(nx.weakly_connected_components(H_full))\n",
    "    print(f\"Found {len(components)} weakly connected components.\")\n",
    "    \n",
    "    # Choose the largest component (or a component with at least 10 nodes)\n",
    "    selected_component = max(components, key=len)\n",
    "    print(f\"Largest component size: {len(selected_component)}\")\n",
    "    \n",
    "    if len(selected_component) < 2:\n",
    "        print(\"No sufficiently connected component found.\")\n",
    "    else:\n",
    "        # Choose a subset (e.g., first 10 nodes) from the selected component.\n",
    "        selected_nodes = sorted(list(selected_component))[:10]\n",
    "        \n",
    "        \n",
    "        # Build a subgraph from the selected nodes.\n",
    "        H_sub = H_full.subgraph(selected_nodes)\n",
    "        \n",
    "        # Prepare labels using the stored node attributes.\n",
    "        labels = {node: H_sub.nodes[node]['label'] for node in H_sub.nodes()}\n",
    "        \n",
    "       \n",
    "else:\n",
    "    print(\"No HPO hierarchy edges available for visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of weakly connected components: 1130\n",
      "Weakly Connected Components Table:\n",
      "   Child  Parent  Edge Weight  Weak Comp ID      Child Label\n",
      "0    p_0  p_6892          1.0             0  f:0.90, ic:0.11\n",
      "1    p_1  p_2375          1.0             1  f:0.90, ic:0.11\n",
      "2    p_1  p_4787          1.0             1  f:0.90, ic:0.11\n",
      "3    p_2  p_4653          1.0             1  f:0.90, ic:0.11\n",
      "4    p_3   p_141          1.0             1  f:0.90, ic:0.11\n",
      "5    p_3  p_1490          1.0             1  f:0.90, ic:0.11\n",
      "6    p_4  p_1369          1.0             1  f:0.90, ic:0.11\n",
      "7    p_6  p_3659          1.0             1  f:0.90, ic:0.11\n",
      "8    p_7  p_1176          1.0             1  f:0.90, ic:0.11\n",
      "9    p_8  p_4499          1.0             1  f:0.90, ic:0.11\n",
      "10   p_9  p_4690          1.0             1  f:0.90, ic:0.11\n",
      "11  p_11     p_5          1.0             1  f:0.90, ic:0.11\n",
      "12  p_11   p_887          1.0             1  f:0.90, ic:0.11\n",
      "13  p_12  p_7422          1.0             1  f:0.90, ic:0.11\n",
      "14  p_13  p_2375          1.0             1  f:0.90, ic:0.11\n",
      "15  p_14  p_3889          1.0             1  f:0.90, ic:0.11\n",
      "16  p_15  p_7632          1.0             1  f:0.90, ic:0.11\n",
      "17  p_15  p_1490          1.0             1  f:0.90, ic:0.11\n",
      "18  p_16   p_100          1.0             1  f:0.90, ic:0.11\n",
      "19  p_17  p_5416          1.0             1  f:0.60, ic:0.51\n",
      "20  p_18  p_5416          1.0             1  f:0.60, ic:0.51\n",
      "21  p_19  p_5098          1.0             1  f:0.60, ic:0.51\n",
      "22  p_21  p_5958          1.0             1  f:0.60, ic:0.51\n",
      "23  p_21  p_2519          1.0             1  f:0.60, ic:0.51\n",
      "24  p_22  p_7879          1.0             1  f:0.60, ic:0.51\n",
      "25  p_22  p_2519          1.0             1  f:0.60, ic:0.51\n",
      "26  p_23  p_7039          1.0             1  f:0.60, ic:0.51\n",
      "27  p_24  p_5230          1.0             1  f:0.60, ic:0.51\n",
      "28  p_25    p_10          1.0             1  f:0.60, ic:0.51\n",
      "29  p_26   p_266          1.0             1  f:0.60, ic:0.51\n",
      "30  p_27   p_887          1.0             1  f:0.60, ic:0.51\n",
      "31  p_28   p_720          1.0             1  f:0.60, ic:0.51\n",
      "32  p_29  p_4653          1.0             1  f:0.60, ic:0.51\n",
      "33  p_29  p_5175          1.0             1  f:0.60, ic:0.51\n",
      "34  p_30  p_4690          1.0             1  f:0.60, ic:0.51\n",
      "35  p_31   p_415          1.0             3  f:0.60, ic:0.51\n",
      "36  p_32    p_10          1.0             1  f:0.60, ic:0.51\n",
      "37  p_33  p_6241          1.0             1  f:0.60, ic:0.51\n",
      "38  p_34  p_3659          1.0             1  f:0.60, ic:0.51\n",
      "39  p_36  p_3889          1.0             1  f:0.60, ic:0.51\n",
      "40  p_38    p_49          1.0             1  f:0.60, ic:0.51\n",
      "41  p_38   p_188          1.0             1  f:0.60, ic:0.51\n",
      "42  p_38  p_7489          1.0             1  f:0.60, ic:0.51\n",
      "43  p_38   p_462          1.0             1  f:0.60, ic:0.51\n",
      "44  p_39  p_5150          1.0             1  f:0.60, ic:0.51\n",
      "45  p_40  p_1569          1.0             1  f:0.20, ic:1.61\n",
      "46  p_41  p_5213          1.0             1  f:0.20, ic:1.61\n",
      "47  p_42  p_1779          1.0             1  f:0.20, ic:1.61\n",
      "48  p_43  p_1072          1.0             1  f:0.20, ic:1.61\n",
      "49  p_43   p_185          1.0             1  f:0.20, ic:1.61\n",
      "Number of strongly connected components: 8600\n",
      "Strongly Connected Components Table:\n",
      "   Child  Parent  Edge Weight  Strong Comp ID      Child Label\n",
      "0    p_0  p_6892          1.0               1  f:0.90, ic:0.11\n",
      "1    p_1  p_2375          1.0               7  f:0.90, ic:0.11\n",
      "2    p_1  p_4787          1.0               7  f:0.90, ic:0.11\n",
      "3    p_2  p_4653          1.0               8  f:0.90, ic:0.11\n",
      "4    p_3   p_141          1.0              16  f:0.90, ic:0.11\n",
      "5    p_3  p_1490          1.0              16  f:0.90, ic:0.11\n",
      "6    p_4  p_1369          1.0              21  f:0.90, ic:0.11\n",
      "7    p_6  p_3659          1.0              24  f:0.90, ic:0.11\n",
      "8    p_7  p_1176          1.0              28  f:0.90, ic:0.11\n",
      "9    p_8  p_4499          1.0              33  f:0.90, ic:0.11\n",
      "10   p_9  p_4690          1.0              35  f:0.90, ic:0.11\n",
      "11  p_11     p_5          1.0              39  f:0.90, ic:0.11\n",
      "12  p_11   p_887          1.0              39  f:0.90, ic:0.11\n",
      "13  p_12  p_7422          1.0              42  f:0.90, ic:0.11\n",
      "14  p_13  p_2375          1.0              43  f:0.90, ic:0.11\n",
      "15  p_14  p_3889          1.0              49  f:0.90, ic:0.11\n",
      "16  p_15  p_7632          1.0              51  f:0.90, ic:0.11\n",
      "17  p_15  p_1490          1.0              51  f:0.90, ic:0.11\n",
      "18  p_16   p_100          1.0              53  f:0.90, ic:0.11\n",
      "19  p_17  p_5416          1.0              56  f:0.60, ic:0.51\n",
      "20  p_18  p_5416          1.0              57  f:0.60, ic:0.51\n",
      "21  p_19  p_5098          1.0              59  f:0.60, ic:0.51\n",
      "22  p_21  p_5958          1.0              64  f:0.60, ic:0.51\n",
      "23  p_21  p_2519          1.0              64  f:0.60, ic:0.51\n",
      "24  p_22  p_7879          1.0              66  f:0.60, ic:0.51\n",
      "25  p_22  p_2519          1.0              66  f:0.60, ic:0.51\n",
      "26  p_23  p_7039          1.0              68  f:0.60, ic:0.51\n",
      "27  p_24  p_5230          1.0              70  f:0.60, ic:0.51\n",
      "28  p_25    p_10          1.0              71  f:0.60, ic:0.51\n",
      "29  p_26   p_266          1.0              72  f:0.60, ic:0.51\n",
      "30  p_27   p_887          1.0              73  f:0.60, ic:0.51\n",
      "31  p_28   p_720          1.0              75  f:0.60, ic:0.51\n",
      "32  p_29  p_4653          1.0              80  f:0.60, ic:0.51\n",
      "33  p_29  p_5175          1.0              80  f:0.60, ic:0.51\n",
      "34  p_30  p_4690          1.0              81  f:0.60, ic:0.51\n",
      "35  p_31   p_415          1.0              83  f:0.60, ic:0.51\n",
      "36  p_32    p_10          1.0              84  f:0.60, ic:0.51\n",
      "37  p_33  p_6241          1.0              87  f:0.60, ic:0.51\n",
      "38  p_34  p_3659          1.0              88  f:0.60, ic:0.51\n",
      "39  p_36  p_3889          1.0              90  f:0.60, ic:0.51\n",
      "40  p_38    p_49          1.0             100  f:0.60, ic:0.51\n",
      "41  p_38   p_188          1.0             100  f:0.60, ic:0.51\n",
      "42  p_38  p_7489          1.0             100  f:0.60, ic:0.51\n",
      "43  p_38   p_462          1.0             100  f:0.60, ic:0.51\n",
      "44  p_39  p_5150          1.0             102  f:0.60, ic:0.51\n",
      "45  p_40  p_1569          1.0             105  f:0.20, ic:1.61\n",
      "46  p_41  p_5213          1.0             109  f:0.20, ic:1.61\n",
      "47  p_42  p_1779          1.0             111  f:0.20, ic:1.61\n",
      "48  p_43  p_1072          1.0             115  f:0.20, ic:1.61\n",
      "49  p_43   p_185          1.0             115  f:0.20, ic:1.61\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# Assume H_full is the directed NetworkX graph we built from the \"is_a\" edges.\n",
    "# If you haven't built H_full already, here's a quick recap using your HPO hierarchy data:\n",
    "if ('phenotype', 'is_a', 'phenotype') in data.edge_index_dict:\n",
    "    hier_edge_index = data['phenotype', 'is_a', 'phenotype'].edge_index\n",
    "    hier_edge_attr = data['phenotype', 'is_a', 'phenotype'].edge_attr\n",
    "    hier_edge_index_np = hier_edge_index.cpu().numpy()\n",
    "    \n",
    "    # Build the full directed graph H_full for phenotype hierarchy.\n",
    "    H_full = nx.DiGraph()\n",
    "    num_nodes = data['phenotype'].num_nodes\n",
    "    for i in range(num_nodes):\n",
    "        freq, ic = data['phenotype'].x[i].tolist()\n",
    "        H_full.add_node(i, label=f\"f:{freq:.2f}, ic:{ic:.2f}\")\n",
    "    \n",
    "    for j in range(hier_edge_index_np.shape[1]):\n",
    "        child = hier_edge_index_np[0, j]\n",
    "        parent = hier_edge_index_np[1, j]\n",
    "        weight = hier_edge_attr[j].item()\n",
    "        H_full.add_edge(child, parent, weight=float(f\"{weight:.4f}\"))\n",
    "else:\n",
    "    print(\"No HPO hierarchy edges available for visualization.\")\n",
    "    H_full = nx.DiGraph()  # Empty graph\n",
    "\n",
    "# ----------------------------\n",
    "# Create a table for weakly connected components\n",
    "\n",
    "# Compute weakly connected components\n",
    "weak_components = list(nx.weakly_connected_components(H_full))\n",
    "print(\"Number of weakly connected components:\", len(weak_components))\n",
    "\n",
    "# Create a mapping from node to its weak component ID\n",
    "node_to_weak_comp = {}\n",
    "for comp_id, comp in enumerate(weak_components):\n",
    "    for node in comp:\n",
    "        node_to_weak_comp[node] = comp_id\n",
    "\n",
    "# Build a list of rows (one per edge) with 5 fields\n",
    "weak_table_rows = []\n",
    "for u, v, d in H_full.edges(data=True):\n",
    "    row = {\n",
    "        \"Child\": f\"p_{u}\",\n",
    "        \"Parent\": f\"p_{v}\",\n",
    "        \"Edge Weight\": d.get(\"weight\", None),\n",
    "        \"Weak Comp ID\": node_to_weak_comp.get(u, None),\n",
    "        \"Child Label\": H_full.nodes[u].get(\"label\", \"\")\n",
    "    }\n",
    "    weak_table_rows.append(row)\n",
    "\n",
    "weak_df = pd.DataFrame(weak_table_rows)\n",
    "print(\"Weakly Connected Components Table:\")\n",
    "print(weak_df.head(50))  # Show first 5 rows\n",
    "\n",
    "# ----------------------------\n",
    "# Create a table for strongly connected components\n",
    "\n",
    "# Compute strongly connected components\n",
    "strong_components = list(nx.strongly_connected_components(H_full))\n",
    "print(\"Number of strongly connected components:\", len(strong_components))\n",
    "\n",
    "# Create a mapping from node to its strong component ID\n",
    "node_to_strong_comp = {}\n",
    "for comp_id, comp in enumerate(strong_components):\n",
    "    for node in comp:\n",
    "        node_to_strong_comp[node] = comp_id\n",
    "\n",
    "strong_table_rows = []\n",
    "for u, v, d in H_full.edges(data=True):\n",
    "    row = {\n",
    "        \"Child\": f\"p_{u}\",\n",
    "        \"Parent\": f\"p_{v}\",\n",
    "        \"Edge Weight\": d.get(\"weight\", None),\n",
    "        \"Strong Comp ID\": node_to_strong_comp.get(u, None),\n",
    "        \"Child Label\": H_full.nodes[u].get(\"label\", \"\")\n",
    "    }\n",
    "    strong_table_rows.append(row)\n",
    "\n",
    "strong_df = pd.DataFrame(strong_table_rows)\n",
    "print(\"Strongly Connected Components Table:\")\n",
    "print(strong_df.head(50))  # Show first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, HeteroConv\n",
    "\n",
    "# Define the improved heterogeneous GAT encoder with add_self_loops disabled.\n",
    "class HeteroGAT(nn.Module):\n",
    "    def __init__(self, metadata, hidden_channels, out_channels, heads=4, dropout=0.3):\n",
    "        \"\"\"\n",
    "        metadata: tuple (node_types, edge_types) from data.metadata()\n",
    "        hidden_channels: hidden dimension for the GAT layers\n",
    "        out_channels: final embedding dimension (for all node types)\n",
    "        heads: number of attention heads\n",
    "        dropout: dropout rate\n",
    "        \"\"\"\n",
    "        super(HeteroGAT, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # First layer: Create a GATConv for each edge type with dropout and no self-loops.\n",
    "        conv_dict = {}\n",
    "        for edge_type in metadata[1]:\n",
    "            conv_dict[edge_type] = GATConv(-1, hidden_channels, heads=heads, concat=True,\n",
    "                                            dropout=dropout, add_self_loops=False)\n",
    "        self.conv1 = HeteroConv(conv_dict, aggr='sum')\n",
    "        \n",
    "        # Second layer: Map from (hidden_channels * heads) to out_channels.\n",
    "        conv_dict2 = {}\n",
    "        for edge_type in metadata[1]:\n",
    "            conv_dict2[edge_type] = GATConv(hidden_channels * heads, out_channels, heads=1, concat=False,\n",
    "                                            dropout=dropout, add_self_loops=False)\n",
    "        self.conv2 = HeteroConv(conv_dict2, aggr='sum')\n",
    "    \n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        # Save input for residual connection\n",
    "        x_dict_in = x_dict\n",
    "        # First GAT layer with ELU activation\n",
    "        x_dict = self.conv1(x_dict, edge_index_dict)\n",
    "        x_dict = {key: F.gelu(x) for key, x in x_dict.items()}\n",
    "        # Optional residual connection (only if dimensions match)\n",
    "        for key in x_dict:\n",
    "            if x_dict_in[key].shape[-1] == x_dict[key].shape[-1]:\n",
    "                x_dict[key] = x_dict[key] + x_dict_in[key]\n",
    "        # Second GAT layer produces final embeddings (call only once)\n",
    "        x_dict = self.conv2(x_dict, edge_index_dict)\n",
    "        return x_dict\n",
    "\n",
    "# Define the heterogeneous autoencoder that wraps the encoder and adds decoders for each node type.\n",
    "class HeteroAutoencoderGAT(nn.Module):\n",
    "    def __init__(self, metadata, hidden_channels, embedding_dim, out_channels_dict, heads=4, dropout=0.3):\n",
    "        \"\"\"\n",
    "        metadata: tuple (node_types, edge_types) from data.metadata()\n",
    "        hidden_channels: hidden dimension for the GAT layers\n",
    "        embedding_dim: final embedding dimension (for all node types)\n",
    "        out_channels_dict: dictionary mapping each node type to its original feature dimension\n",
    "        heads: number of attention heads\n",
    "        dropout: dropout rate\n",
    "        \"\"\"\n",
    "        super(HeteroAutoencoderGAT, self).__init__()\n",
    "        self.encoder = HeteroGAT(metadata, hidden_channels, embedding_dim, heads=heads, dropout=dropout)\n",
    "        # Create a decoder (linear layer) for each node type to reconstruct the original features.\n",
    "        self.decoders = nn.ModuleDict()\n",
    "        for node_type in metadata[0]:\n",
    "            self.decoders[node_type] = nn.Linear(embedding_dim, out_channels_dict[node_type])\n",
    "    \n",
    "    def forward(self, data):\n",
    "        # Encode: Get embeddings for each node type.\n",
    "        x_dict = self.encoder(data.x_dict, data.edge_index_dict)\n",
    "        # Decode: Reconstruct original features for each node type.\n",
    "        reconstructions = {}\n",
    "        for node_type, x in x_dict.items():\n",
    "            reconstructions[node_type] = self.decoders[node_type](x)\n",
    "        return x_dict, reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/1000, Loss: 0.2191\n",
      "Epoch 100/1000, Loss: 0.2073\n",
      "Epoch 150/1000, Loss: 0.2039\n",
      "Epoch 200/1000, Loss: 0.2016\n",
      "Epoch 250/1000, Loss: 0.1970\n",
      "Epoch 300/1000, Loss: 0.1966\n",
      "Epoch 350/1000, Loss: 0.1958\n",
      "Epoch 400/1000, Loss: 0.1957\n",
      "Epoch 450/1000, Loss: 0.1969\n",
      "Epoch 500/1000, Loss: 0.1953\n",
      "Epoch 550/1000, Loss: 0.1963\n",
      "Epoch 600/1000, Loss: 0.1959\n",
      "Epoch 650/1000, Loss: 0.1940\n",
      "Epoch 700/1000, Loss: 0.1934\n",
      "Epoch 750/1000, Loss: 0.1951\n",
      "Epoch 800/1000, Loss: 0.1947\n",
      "Epoch 850/1000, Loss: 0.1950\n",
      "Epoch 900/1000, Loss: 0.1959\n",
      "Epoch 950/1000, Loss: 0.1946\n",
      "Epoch 1000/1000, Loss: 0.1947\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define original feature dimensions:\n",
    "# - Phenotype nodes have 2 features: [frequency, IC]\n",
    "# - Disease nodes have 2 features (dummy features)\n",
    "out_channels_dict = {'phenotype': 2, 'disease': 2}\n",
    "\n",
    "# Get metadata from the heterogeneous graph (assumed that 'data' is already constructed)\n",
    "metadata = data.metadata()  # Returns (node_types, edge_types)\n",
    "\n",
    "# Initialize the improved GAT-based autoencoder model.\n",
    "hidden_channels = 32    # Increased hidden dimension for richer representations\n",
    "embedding_dim = 32      # Final embedding dimension\n",
    "heads = 4\n",
    "dropout = 0.3\n",
    "model = HeteroAutoencoderGAT(metadata, hidden_channels, embedding_dim, out_channels_dict, heads=heads, dropout=dropout)\n",
    "\n",
    "# Set up the optimizer and MSE loss criterion.\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop: Train the autoencoder to reconstruct node features.\n",
    "model.train()\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    x_dict, reconstructions = model(data)\n",
    "    loss = 0\n",
    "    # Sum reconstruction loss over each node type.\n",
    "    for node_type in data.node_types:\n",
    "        loss += criterion(reconstructions[node_type], data[node_type].x)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phenotype embeddings shape: torch.Size([8600, 32])\n",
      "disease embeddings shape: torch.Size([4283, 32])\n",
      "Heterogeneous node embeddings (improved GAT) saved as 'hetero_node_embeddings_gat_improved.pt'\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode and extract embeddings from the encoder.\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    embeddings = model.encoder(data.x_dict, data.edge_index_dict)\n",
    "\n",
    "# Print the shape of embeddings for each node type.\n",
    "for node_type, emb in embeddings.items():\n",
    "    print(f\"{node_type} embeddings shape: {emb.shape}\")\n",
    "\n",
    "# Save the learned embeddings to a file.\n",
    "torch.save(embeddings, \"hetero_node_embeddings_gat_improved.pt\")\n",
    "print(\"Heterogeneous node embeddings (improved GAT) saved as 'hetero_node_embeddings_gat_improved.pt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0123,  0.5178,  0.2641, -0.0134, -0.0714,  0.2372,  0.1069,  0.0727,\n",
       "        -0.0856,  0.2326, -0.3496, -0.0773, -0.0644,  0.1225, -0.1244, -0.2612,\n",
       "        -0.2071,  0.0653, -0.3245,  0.2696, -0.0826, -0.1368,  0.0166,  0.1902,\n",
       "         0.4661,  0.4707, -0.3918, -0.2464, -0.1001, -0.2758, -0.2370,  0.4765])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings['phenotype'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0924,  0.4248,  0.3055,  0.0094,  0.1309,  0.2433, -0.2703, -0.4354,\n",
       "        -0.3345, -0.0201,  0.0618, -0.3377, -0.2793,  0.0684, -0.2060, -0.3527,\n",
       "         0.1253,  0.4266,  0.0026,  0.0421, -0.2610,  0.3692, -0.0695,  0.2371,\n",
       "         0.0031,  0.0260,  0.0939, -0.4155, -0.1748,  0.2760,  0.0548,  0.0721])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings['disease'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RL Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym\n",
      "  Using cached gym-0.26.2-py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.18.0 in /Users/sriramsohan/miniforge3/envs/RL_env/lib/python3.10/site-packages (from gym) (2.2.3)\n",
      "Collecting cloudpickle>=1.2.0 (from gym)\n",
      "  Using cached cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gym_notices>=0.0.4 (from gym)\n",
      "  Using cached gym_notices-0.0.8-py3-none-any.whl.metadata (1.0 kB)\n",
      "Using cached cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Using cached gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Installing collected packages: gym_notices, cloudpickle, gym\n",
      "Successfully installed cloudpickle-3.1.1 gym-0.26.2 gym_notices-0.0.8\n"
     ]
    }
   ],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from gym import spaces\n",
    "\n",
    "class RewardFn():\n",
    "    \"\"\"\n",
    "    Computes reward as: -alpha(t) * L2_distance, where:\n",
    "      alpha(t) = alpha_base + alpha_scale * (t / t_max)\n",
    "    If L2_distance < threshold, 'done' is set to True.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_steps=22, threshold=0.5):\n",
    "        self.max_steps = max_steps\n",
    "        self.alpha_base = 0.01\n",
    "        self.alpha_scale = 1.0\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def calculate_reward(self, emb_hat, emb, t):\n",
    "        distance = torch.dist(emb_hat, emb, p=2)\n",
    "        done = distance < self.threshold\n",
    "        alpha = self.alpha_base + self.alpha_scale * (t / self.max_steps)\n",
    "        reward = -alpha * distance\n",
    "        return reward.item(), done\n",
    "\n",
    "class DifferentialDiagnosisEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    RL environment for differential diagnosis.\n",
    "    \n",
    "    State:\n",
    "      - A 32-dimensional vector representing the patient's phenotype profile.\n",
    "    \n",
    "    Action:\n",
    "      - A discrete action corresponding to a candidate phenotype query.\n",
    "    \n",
    "    Reward:\n",
    "      - Negative L2 distance between the updated state and a target disease embedding,\n",
    "        scaled by a time-dependent factor, with a termination condition if the distance is below a threshold.\n",
    "    \n",
    "    Episode Termination:\n",
    "      - After a fixed number of steps (max_steps) or if the state is close enough to the target.\n",
    "    \"\"\"\n",
    "    def __init__(self, phenotype_embeddings, disease_embeddings, candidate_actions, max_steps=10, threshold=0.5):\n",
    "        super(DifferentialDiagnosisEnv, self).__init__()\n",
    "        # phenotype_embeddings: numpy array of shape (num_phenotypes, 32)\n",
    "        # disease_embeddings: numpy array of shape (num_diseases, 32)\n",
    "        self.phenotype_embeddings = phenotype_embeddings\n",
    "        self.disease_embeddings = disease_embeddings\n",
    "        self.candidate_actions = candidate_actions  # List of candidate phenotype indices\n",
    "        self.max_steps = max_steps\n",
    "        self.threshold = threshold\n",
    "        \n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(32,), dtype=np.float32)\n",
    "        self.action_space = spaces.Discrete(len(candidate_actions))\n",
    "        \n",
    "        self.current_step = 0\n",
    "        self.target_disease_index = 0  # For demonstration, assume the target disease is at index 0.\n",
    "        self.state = None\n",
    "        \n",
    "        self.reward_fn = RewardFn(max_steps=self.max_steps, threshold=self.threshold)\n",
    "\n",
    "    def reset(self):\n",
    "        # Initialize the state by averaging 5 random phenotype embeddings.\n",
    "        indices = np.random.choice(self.phenotype_embeddings.shape[0], 5, replace=False)\n",
    "        self.state = np.mean(self.phenotype_embeddings[indices], axis=0)\n",
    "        self.current_step = 0\n",
    "        return self.state.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        # Validate action\n",
    "        if action < 0 or action >= len(self.candidate_actions):\n",
    "            raise ValueError(\"Invalid action.\")\n",
    "        \n",
    "        # Retrieve the embedding corresponding to the chosen candidate query.\n",
    "        query_embedding = self.phenotype_embeddings[self.candidate_actions[action]]\n",
    "        \n",
    "        # Update the state: add a fraction (alpha factor) of the query embedding.\n",
    "        update_factor = 0.1  # You may tune this parameter.\n",
    "        self.state = self.state + update_factor * query_embedding\n",
    "        \n",
    "        # Compute reward: use the dynamic reward function.\n",
    "        target = self.disease_embeddings[self.target_disease_index]\n",
    "        state_tensor = torch.tensor(self.state, dtype=torch.float)\n",
    "        target_tensor = torch.tensor(target, dtype=torch.float)\n",
    "        reward, done_by_threshold = self.reward_fn.calculate_reward(state_tensor, target_tensor, self.current_step)\n",
    "        \n",
    "        self.current_step += 1\n",
    "        done = done_by_threshold or (self.current_step >= self.max_steps)\n",
    "        info = {}\n",
    "        return self.state.astype(np.float32), reward, done, info\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        print(f\"Step: {self.current_step}, State: {self.state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting shimmy>=2.0\n",
      "  Using cached Shimmy-2.0.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /Users/sriramsohan/miniforge3/envs/RL_env/lib/python3.10/site-packages (from shimmy>=2.0) (2.2.3)\n",
      "Requirement already satisfied: gymnasium>=1.0.0a1 in /Users/sriramsohan/miniforge3/envs/RL_env/lib/python3.10/site-packages (from shimmy>=2.0) (1.0.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/sriramsohan/miniforge3/envs/RL_env/lib/python3.10/site-packages (from gymnasium>=1.0.0a1->shimmy>=2.0) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /Users/sriramsohan/miniforge3/envs/RL_env/lib/python3.10/site-packages (from gymnasium>=1.0.0a1->shimmy>=2.0) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Users/sriramsohan/miniforge3/envs/RL_env/lib/python3.10/site-packages (from gymnasium>=1.0.0a1->shimmy>=2.0) (0.0.4)\n",
      "Using cached Shimmy-2.0.0-py3-none-any.whl (30 kB)\n",
      "Installing collected packages: shimmy\n",
      "Successfully installed shimmy-2.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install 'shimmy>=2.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 10       |\n",
      "|    ep_rew_mean     | -11.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 6508     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 10          |\n",
      "|    ep_rew_mean          | -11.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4596        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 0           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013611449 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.6        |\n",
      "|    explained_variance   | -0.106      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.33        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0211     |\n",
      "|    value_loss           | 9.52        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 10          |\n",
      "|    ep_rew_mean          | -11.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4167        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014903968 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.57       |\n",
      "|    explained_variance   | 0.09        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.92        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0218     |\n",
      "|    value_loss           | 5.67        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 10          |\n",
      "|    ep_rew_mean          | -11.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3966        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014471345 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.55       |\n",
      "|    explained_variance   | 0.48        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.697       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0256     |\n",
      "|    value_loss           | 2.71        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 10          |\n",
      "|    ep_rew_mean          | -11.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3884        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013941623 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.54       |\n",
      "|    explained_variance   | 0.82        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.179       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0214     |\n",
      "|    value_loss           | 0.874       |\n",
      "-----------------------------------------\n",
      "RL agent trained and saved as 'ppo_differential_diagnosis'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "embeddings = torch.load(\"hetero_node_embeddings_gat_improved.pt\")\n",
    "phenotype_embeddings = embeddings['phenotype'].cpu().numpy()  # Shape: (8600, 32)\n",
    "disease_embeddings = embeddings['disease'].cpu().numpy()      # Shape: (4283, 32)\n",
    "\n",
    "candidate_actions = list(range(100))\n",
    "\n",
    "# Create the custom RL environment.\n",
    "env = DifferentialDiagnosisEnv(phenotype_embeddings, disease_embeddings, candidate_actions, max_steps=10, threshold=0.5)\n",
    "\n",
    "# Create the PPO agent with an MLP policy.\n",
    "model_rl = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# Train the agent for a specified number of timesteps (adjust as needed).\n",
    "model_rl.learn(total_timesteps=10000)\n",
    "\n",
    "# Save the trained RL model.\n",
    "model_rl.save(\"ppo_differential_diagnosis\")\n",
    "print(\"RL agent trained and saved as 'ppo_differential_diagnosis'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
